{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Created by Diego Alvarez-Estevez, 2025\n",
    "\n",
    "# This code is part of a Qiskit project.\n",
    "#\n",
    "# (C) Copyright IBM 2021, 2024.\n",
    "#\n",
    "# This code is licensed under the Apache License, Version 2.0. You may\n",
    "# obtain a copy of this license in the LICENSE.txt file in the root directory\n",
    "# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "#\n",
    "# Any modifications or derivative works of this code must retain this\n",
    "# copyright notice, and modified files need to carry a notice indicating\n",
    "# that they have been altered from the originals.\n",
    "\n",
    "# Using IRIS dataset\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_algorithms.optimizers import SPSA\n",
    "\n",
    "from qiskit_machine_learning.kernels import TrainableFidelityStatevectorKernel, FidelityStatevectorKernel # more efficient for statevector based simulators (better even than previous with Aer primitives)\n",
    "from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit_machine_learning.utils.loss_functions import SVCLoss\n",
    "\n",
    "from common_utils import *\n",
    "\n",
    "# INITIALIZATION PARAMETERS\n",
    "random_seed = 12345\n",
    "algorithm_globals.random_seed = random_seed\n",
    "num_repetitions = 30\n",
    "num_QKT_iter = 400\n",
    "\n",
    "scale_inputs = True\n",
    "class_weight = None # None is default, assumes equeal class importance in binary setting, \n",
    "\n",
    "minimize_display = True\n",
    "\n",
    "# PREPROCESSING DATSET\n",
    "feature_dim = 4\n",
    "poly_degree=7 #4 for n=2, 6-7 for n=4\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# Take all features\n",
    "selected_features = range(feature_dim)\n",
    "x_full = iris.data[:, selected_features]\n",
    "y_full = iris.target\n",
    "\n",
    "# Select problem version\n",
    "linear_version = True\n",
    "if linear_version:\n",
    "    class_to_remove = 2 # Select two from [0,1,2]\n",
    "    # select 2 to resemble https://pennylane.ai/qml/demos/tutorial_kernel_based_training/ -> setosa vs versicolor, linearly separable\n",
    "    filename_output = 'results_IRIS_linear.pkl'\n",
    "else:\n",
    "    # non-linear version\n",
    "    class_to_remove = 0 # Select two from [0,1,2]\n",
    "    # select 0 versicolor vs virginica -> non-linearly separable (slightly)\n",
    "    filename_output = 'results_IRIS_nonlinear.pkl'\n",
    "\n",
    "# Convert to binary problem\n",
    "y_full = y_full[np.where(iris.target != class_to_remove)]\n",
    "x_full = x_full[np.where(iris.target != class_to_remove)]\n",
    "\n",
    "# Transform labels to [1, -1]\n",
    "min_value = np.min(y_full)\n",
    "y_temp = y_full.copy()\n",
    "y_temp[np.where(y_full == min_value)] = -1\n",
    "y_temp[np.where(y_full != min_value)] = 1\n",
    "y_full = y_temp.copy()\n",
    "\n",
    "# Initialize structure to store benchmark results\n",
    "results = {\n",
    "    \"LR\": [],\n",
    "    \"SVM_linear\": [],\n",
    "    \"SVM_poly\": [],\n",
    "    \"SVM_rbf\": [],\n",
    "    \"qSVM_ZZ\": [],\n",
    "    \"qSVM_COV\": [],\n",
    "    \"qSVM_ZZ_opt_s\": [],\n",
    "    \"qSVM_COV_opt_s\": [],\n",
    "    \"qSVM_ZZ_opt_d\": [],\n",
    "    \"qSVM_COV_opt_d\": []\n",
    "}\n",
    "results_hyper_params = {\n",
    "    \"LR\": [],\n",
    "    \"SVM_linear\": [],\n",
    "    \"SVM_poly\": [],\n",
    "    \"SVM_rbf\": [],\n",
    "    \"qSVM_ZZ\": [],\n",
    "    \"qSVM_COV\": []\n",
    "}\n",
    "results_loss = {\n",
    "    \"qSVM_ZZ_opt_s\": [],\n",
    "    \"qSVM_COV_opt_s\": [],\n",
    "    \"qSVM_ZZ_opt_d\": [],\n",
    "    \"qSVM_COV_opt_d\": []\n",
    "}\n",
    "\n",
    "# Set default verbose for GridSearchCV\n",
    "used_verbose = 3\n",
    "if minimize_display == True:\n",
    "    used_verbose = 0\n",
    "\n",
    "for rep in range(num_repetitions):\n",
    "    print(f\"REPETITION {rep+1} out of {num_repetitions}\")\n",
    "    \n",
    "    # Random stratified sample using 70% for TR/VAL and 30% for TS\n",
    "    xtrain_rep, xtest_rep, ytrain_rep, ytest_rep = train_test_split(x_full, y_full, test_size=0.3, random_state=random_seed, stratify=y_full)\n",
    "\n",
    "    # Visualize data\n",
    "    if not(minimize_display):\n",
    "        print(\"xtrain_rep: \", xtrain_rep)\n",
    "        print(\"ytrain_rep: \", ytrain_rep)\n",
    "        print(\"xtest_rep: \", xtrain_rep)\n",
    "        print(\"ytest_rep: \", ytrain_rep)\n",
    "        \n",
    "    print(\"Train set distribution:\")\n",
    "    count_labels(ytrain_rep)\n",
    "    print(\"Test set distribution:\")\n",
    "    count_labels(ytest_rep)\n",
    "\n",
    "    if random_seed is not None:\n",
    "        random_seed += 1  # Increment seed to get different samples in each repetition\n",
    "\n",
    "    ## ALL MODELS (CLASSICAL AND QUANTUM)\n",
    "    \n",
    "    # Define hyperparameter space\n",
    "    C_range = np.logspace(-2, 2, 5);print(\"C_range:\", C_range) # Returns numbers evenly space in log scale. Params (start, stop, num), with default base=10\n",
    "    gamma_range = np.logspace(-3, 1, 5);print(\"Gamma_range:\", gamma_range)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    ## CLASSICAL MODELS\n",
    "       \n",
    "    # Input scaling. For classical models standardize data. \n",
    "    # Note for this dataset features are in [0, 2PI] rage by default\n",
    "    if scale_inputs == True:\n",
    "        scaler = StandardScaler()\n",
    "        xtrain = scaler.fit_transform(xtrain_rep)\n",
    "        xtest = scaler.transform(xtest_rep)\n",
    "    \n",
    "    # CLASSICAL APPROACH (Logistic Regression) with hyperparameter search\n",
    "    param_grid = dict(C=C_range)\n",
    "    grid = GridSearchCV(LogisticRegression(class_weight=class_weight), param_grid=param_grid, cv=cv, verbose=used_verbose, refit=True) \n",
    "    \n",
    "    start = time.time()\n",
    "    grid.fit(xtrain, ytrain_rep) # Hence generating kind of train/val sets, keeping test data completely independent of this hyperparamter search\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Classical LR hyperparameter search time: {round(elapsed,2)} seconds\")\n",
    "    print(\n",
    "        \"The best parameters are %s with a score of %0.2f\"\n",
    "        % (grid.best_params_, grid.best_score_)\n",
    "    )\n",
    "    ypred_tr = grid.predict(xtrain)\n",
    "    print(f\"Classical LR with BEST hyperparamters (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred_tr),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred_tr),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred_tr),2)}\")\n",
    "    ypred_ts = grid.predict(xtest)\n",
    "    print(f\"Classical LR with BEST hyperparamters (acc test): {round(metrics.accuracy_score(ytest_rep, ypred_ts),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred_ts),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred_ts),2)}\")\n",
    "    # Obtain corresponding test metrics for final evaluation\n",
    "    rep_metrics = compute_metrics(ytrain_rep, ypred_tr, ytest_rep, ypred_ts)\n",
    "    results[\"LR\"].append(rep_metrics)\n",
    "    results_hyper_params[\"LR\"].append(grid.best_params_)\n",
    "    \n",
    "    # CLASSICAL APPROACH (SVC, with linear kernel => no kernel trick) with hyperparameter search\n",
    "    param_grid = dict(C=C_range)\n",
    "    grid = GridSearchCV(SVC(kernel=\"linear\", class_weight=class_weight), param_grid=param_grid, cv=cv, verbose=used_verbose, refit=True)\n",
    "\n",
    "    start = time.time()\n",
    "    grid.fit(xtrain, ytrain_rep) # Hence generating kind of train/val sets, keeping test data completely independent of this hyperparamter search\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Classical SVM hyperparameter search time (linear): {round(elapsed,2)} seconds\")\n",
    "    print(\n",
    "        \"The best parameters are %s with a score of %0.2f\"\n",
    "        % (grid.best_params_, grid.best_score_)\n",
    "    )\n",
    "    ypred_tr = grid.predict(xtrain)\n",
    "    print(f\"Classical SVC -LINEAR kernel with BEST hyperparamters- (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred_tr),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred_tr),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred_tr),2)}\")\n",
    "    ypred_ts = grid.predict(xtest)\n",
    "    print(f\"Classical SVC -LINEAR kernel with BEST hyperparamters- (acc test): {round(metrics.accuracy_score(ytest_rep, ypred_ts),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred_ts),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred_ts),2)}\")\n",
    "    # Obtain corresponding test metrics for final evaluation\n",
    "    rep_metrics = compute_metrics(ytrain_rep, ypred_tr, ytest_rep, ypred_ts)\n",
    "    results[\"SVM_linear\"].append(rep_metrics)\n",
    "    results_hyper_params[\"SVM_linear\"].append(grid.best_params_)\n",
    "    \n",
    "    # CLASSICAL APPROACH (SVC, with polynomial kernel) with hyperparameter search\n",
    "    param_grid = dict(C=C_range)\n",
    "    grid = GridSearchCV(SVC(kernel=\"poly\", degree=poly_degree, class_weight=class_weight), param_grid=param_grid, cv=cv, verbose=used_verbose, refit=True)\n",
    "\n",
    "    start = time.time()\n",
    "    grid.fit(xtrain, ytrain_rep) # Hence generating kind of train/val sets, keeping test data completely independent of this hyperparamter search\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Classical SVM hyperparameter search time (poly): {round(elapsed,2)} seconds\")\n",
    "    print(\n",
    "        \"The best parameters are %s with a score of %0.2f\"\n",
    "        % (grid.best_params_, grid.best_score_)\n",
    "    )\n",
    "    ypred_tr = grid.predict(xtrain)\n",
    "    print(f\"Classical SVC -POLY kernel with BEST hyperparamters- (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred_tr),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred_tr),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred_tr),2)}\")\n",
    "    ypred_ts = grid.predict(xtest)\n",
    "    print(f\"Classical SVC -POLY kernel with BEST hyperparamters- (acc test): {round(metrics.accuracy_score(ytest_rep, ypred_ts),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred_ts),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred_ts),2)}\")\n",
    "    # Obtain corresponding test metrics for final evaluation\n",
    "    rep_metrics = compute_metrics(ytrain_rep, ypred_tr, ytest_rep, ypred_ts)\n",
    "    results[\"SVM_poly\"].append(rep_metrics)\n",
    "    results_hyper_params[\"SVM_poly\"].append(grid.best_params_)\n",
    "    \n",
    "    # CLASSICAL APPROACH (SVC, RBF) with hyperparameter search\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)  \n",
    "    grid = GridSearchCV(SVC(kernel=\"rbf\", class_weight=class_weight), param_grid=param_grid, cv=cv, verbose=used_verbose, refit=True) \n",
    "    \n",
    "    start = time.time()\n",
    "    grid.fit(xtrain, ytrain_rep) # Hence generating kind of train/val sets, keeping test data completely independent of this hyperparamter search\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Classical SVM hyperparameter search time (rbf): {round(elapsed,2)} seconds\")\n",
    "    print(\n",
    "        \"The best parameters are %s with a score of %0.2f\"\n",
    "        % (grid.best_params_, grid.best_score_)\n",
    "    )\n",
    "    ypred_tr = grid.predict(xtrain)\n",
    "    print(f\"Classical SVC -rbf kernel with BEST hyperparamters- (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred_tr),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred_tr),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred_tr),2)}\")\n",
    "    ypred_ts = grid.predict(xtest)\n",
    "    print(f\"Classical SVC -rbf kernel with BEST hyperparamters- (acc test): {round(metrics.accuracy_score(ytest_rep, ypred_ts),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred_ts),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred_ts),2)}\")\n",
    "    # Obtain corresponding test metrics for final evaluation\n",
    "    rep_metrics = compute_metrics(ytrain_rep, ypred_tr, ytest_rep, ypred_ts)\n",
    "    results[\"SVM_rbf\"].append(rep_metrics)\n",
    "    results_hyper_params[\"SVM_rbf\"].append(grid.best_params_)\n",
    "    \n",
    "    ## QUANTUM KERNEL METHODS\n",
    "    \n",
    "    # Input scaling. \n",
    "    # Note for this dataset features are in [0, 2PI] rage by default\n",
    "    if scale_inputs == True:\n",
    "        scaler = MinMaxScaler(feature_range=(0,2*np.pi))\n",
    "        xtrain = scaler.fit_transform(xtrain_rep)\n",
    "        xtest = scaler.transform(xtest_rep)\n",
    "\n",
    "    # List of tested quantum feature maps\n",
    "    quantum_feature_maps = [\"Covariant\", \"ZZ\"]\n",
    "    \n",
    "    for qfeat_map in quantum_feature_maps:\n",
    "        print(f\"\\nQuantum Feature Map: {qfeat_map}\")\n",
    "\n",
    "        if qfeat_map == \"Covariant\":\n",
    "            # Covariant Feature Map\n",
    "            adhoc_feature_map = CovariantFeatureMap(feature_dim, reps=1)\n",
    "        elif qfeat_map == \"ZZ\":\n",
    "            # ZZFeatureMap\n",
    "            adhoc_feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=2, insert_barriers=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid quantum feature map\")\n",
    "\n",
    "        # Print chosen feature map\n",
    "        if minimize_display == False:\n",
    "            display(adhoc_feature_map.draw(\"mpl\"))\n",
    "            display(adhoc_feature_map.decompose().draw(\"mpl\"))\n",
    "        \n",
    "        # QSVC hyperparameter search (C and \"scaling factor / bandwidth\")\n",
    "        scaling_factor_range = [0.001, 0.01, 0.1, 0.5, 1.0];print(\"qKernel_scaling_factor_range:\", scaling_factor_range)\n",
    "        best_scaling_factor = None\n",
    "        best_score = None\n",
    "        for scaling_factor in scaling_factor_range:\n",
    "            \n",
    "            # Apply scaling factor (see \"importance of kernel bandwith\" paper)\n",
    "            xtrain_scaled = xtrain * scaling_factor \n",
    "            xtest_scaled = xtest * scaling_factor\n",
    "\n",
    "            print(f\"\\nQuantum Kernel with scaling factor: {scaling_factor} for range {scaling_factor_range}\")\n",
    "                \n",
    "            # Computing (fidelity) kernel matrix entries\n",
    "            # Using statevector simulation\n",
    "            adhoc_kernel = FidelityStatevectorKernel(feature_map=adhoc_feature_map)\n",
    "    \n",
    "            # Calculate and plot resulting quantum kernel\n",
    "            start = time.time()\n",
    "            adhoc_matrix_train = adhoc_kernel.evaluate(x_vec=xtrain_scaled)\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"Quantum kernel matrix (training) time: {round(elapsed,2)} seconds\")\n",
    "            start = time.time()\n",
    "            adhoc_matrix_test = adhoc_kernel.evaluate(x_vec=xtest_scaled, y_vec=xtrain_scaled)\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"Quantum kernel matrix (test) time: {round(elapsed,2)} seconds\")\n",
    "            \n",
    "            if minimize_display == False:\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                axs[0].imshow(\n",
    "                    np.asmatrix(adhoc_matrix_train), interpolation=\"nearest\", origin=\"upper\", cmap=\"Blues\"\n",
    "                )\n",
    "                axs[0].set_title(\"Ad hoc training kernel matrix\")\n",
    "                axs[1].imshow(np.asmatrix(adhoc_matrix_test), interpolation=\"nearest\", origin=\"upper\", cmap=\"Reds\")\n",
    "                axs[1].set_title(\"Ad hoc testing kernel matrix\")\n",
    "                plt.show()\n",
    "    \n",
    "            # Evaluate QSVC hyperparameter \"C\"\n",
    "            param_grid = dict(C=C_range)  \n",
    "            grid = GridSearchCV(SVC(kernel=\"precomputed\", class_weight=class_weight), param_grid=param_grid, cv=cv, verbose=used_verbose, refit=True)\n",
    "        \n",
    "            start = time.time()\n",
    "            grid.fit(adhoc_matrix_train, ytrain_rep) # Hence generating kind of train/val sets, keeping test data completely independent of this hyperparamter search\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"Quantum SVM hyperparameter search time (precomputed): {round(elapsed,2)} seconds\")\n",
    "            print(\n",
    "                \"The best parameters are %s with a score of %0.2f\"\n",
    "                % (grid.best_params_, grid.best_score_)\n",
    "            )\n",
    "            ypred_tr = grid.predict(adhoc_matrix_train)\n",
    "            print(f\"QSVC with BEST hyperparamters (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred_tr),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred_tr),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred_tr),2)}\")\n",
    "            ypred_ts = grid.predict(adhoc_matrix_test)\n",
    "            print(f\"QSVC with BEST hyperparamters (acc test): {round(metrics.accuracy_score(ytest_rep, ypred_ts),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred_ts),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred_ts),2)}\")\n",
    "        \n",
    "            # Evaluate \"scaling_factor\" hyperparameter\n",
    "            current_score = grid.best_score_ # Accuracy score is default used within GridSearchCV\n",
    "            if best_score == None:\n",
    "                best_score = current_score\n",
    "                best_scaling_factor = scaling_factor\n",
    "            else:\n",
    "                if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_scaling_factor = scaling_factor\n",
    "                    best_C = grid.best_params_[\"C\"]\n",
    "                    # Obtain corresponding test metrics for final evaluation\n",
    "                    rep_metrics = compute_metrics(ytrain_rep, ypred_tr, ytest_rep, ypred_ts)\n",
    "\n",
    "        # Store final results for this repetition\n",
    "        if qfeat_map == \"Covariant\":\n",
    "            # Covariant Feature Map\n",
    "            results[\"qSVM_COV\"].append(rep_metrics)\n",
    "            results_hyper_params[\"qSVM_COV\"].append({\"C\":best_C,\"scaling_factor\":best_scaling_factor})\n",
    "        elif qfeat_map == \"ZZ\":\n",
    "            # ZZFeatureMap\n",
    "            results[\"qSVM_ZZ\"].append(rep_metrics)\n",
    "            results_hyper_params[\"qSVM_ZZ\"].append({\"C\":best_C,\"scaling_factor\":best_scaling_factor})\n",
    "        else:\n",
    "            raise ValueError(\"Invalid quantum feature map\")\n",
    "                \n",
    "        # QSVC + QKA (kernel training)\n",
    "\n",
    "        # Set best found hyperparameters\n",
    "        xtrain_scaled = xtrain * best_scaling_factor \n",
    "        xtest_scaled = xtest * best_scaling_factor\n",
    "        \n",
    "        print(f\"\\nTraining Quantum Kernel with best hypeparameters C: {best_C} and scaling factor: {best_scaling_factor}\")\n",
    "        \n",
    "        for qkt_single_training_parameter in [True, False]:\n",
    "\n",
    "            print(f\"Shared configuration: {qkt_single_training_parameter}\")\n",
    "            \n",
    "            # Create trainable feature map\n",
    "            if qkt_single_training_parameter == True:\n",
    "                training_params = ParameterVector(\"θ\", 3)   \n",
    "            else:\n",
    "                training_params = ParameterVector(\"θ\", adhoc_feature_map.num_qubits*3)\n",
    "            #print(len(training_params))\n",
    "            init_point = np.zeros(len(training_params)) # Initial point 0 => no rotation => original kernel. Change if other desired\n",
    "            fm = TrainableFeatureMap(adhoc_feature_map, training_params, qkt_single_training_parameter)\n",
    "        \n",
    "            # Set up calculation of parameterized (fidelity) kernel matrix entries\n",
    "            # Using statevector simulation\n",
    "            quant_kernel = TrainableFidelityStatevectorKernel(feature_map=fm, training_parameters=training_params)\n",
    "        \n",
    "            # Set up the optimizer and loss function\n",
    "            cb_qkt = QKTCallback()\n",
    "            spsa_opt = SPSA(maxiter=num_QKT_iter, callback=cb_qkt.callback, blocking=True, second_order=True)\n",
    "        \n",
    "            # We provide best found C hyperparameter\n",
    "            user_loss = SVCLoss(C=best_C, class_weight=class_weight)\n",
    "        \n",
    "            # Check correspondence with previous \"baseline\" value obtained (should match if init_point = 0, meaning no rotations applied) U(θ,0,0)=RY(θ)\n",
    "            loss_value_baseline = user_loss.evaluate(init_point, quant_kernel, xtrain_scaled, ytrain_rep)\n",
    "            print(f\"QSVC associated (baseline trainable) train loss (using {user_loss} evaluate): {round(loss_value_baseline,4)}\")\n",
    "        \n",
    "            # Instantiate a quantum kernel trainer\n",
    "            qkt = QuantumKernelTrainer(\n",
    "                quantum_kernel=quant_kernel, loss=user_loss, optimizer=spsa_opt, initial_point=init_point\n",
    "            )\n",
    "        \n",
    "            # Train the kernel using QKT directly\n",
    "            start = time.time()\n",
    "            algorithm_globals.random_seed = random_seed\n",
    "            qka_results = qkt.fit(xtrain_scaled, ytrain_rep)\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"QKT training time: {round(elapsed,2)} seconds\")\n",
    "\n",
    "            # Retrieve optimization results\n",
    "            optimized_kernel = qka_results.quantum_kernel\n",
    "            plot_data = cb_qkt.get_callback_data()  # callback data\n",
    "\n",
    "            # Notice the \"optimal_point\" shown before corresponds with last evaluation point\n",
    "            # WHICH MIGHT NOT CORRESPOND TO REAL optimization minimum!!!\n",
    "            # Later in the code, the minimal (theoretical best) point is used\n",
    "            \n",
    "            if minimize_display == False:\n",
    "                print(qka_results)\n",
    "                \n",
    "                # Calculate and show results corresponding to \"end point\"\n",
    "                print(f\"QKA[end_point], loss={round(plot_data[2][-1],4)}, params={plot_data[1][-1]}\")\n",
    "                \n",
    "                # Use QSVC for classification\n",
    "                qsvc = QSVC(quantum_kernel=optimized_kernel, C=best_C, class_weight=class_weight)\n",
    "                \n",
    "                # Fit the QSVC\n",
    "                start = time.time()\n",
    "                qsvc.fit(xtrain_scaled, ytrain_rep)\n",
    "                elapsed = time.time() - start\n",
    "                print(f\"QSVC + QKA[end_point] optimized kernel training time: {round(elapsed,2)} seconds\")\n",
    "                \n",
    "                ypred = qsvc.predict(xtrain_scaled)\n",
    "                print(f\"QSVC+QKA[end_point] (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred),2)}\")\n",
    "                ypred = qsvc.predict(xtest_scaled)\n",
    "                print(f\"QSVC+QKA[end_point] (acc test): {round(metrics.accuracy_score(ytest_rep, ypred),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred),2)}\")\n",
    "                \n",
    "                K = optimized_kernel.evaluate(xtrain_scaled)  # kernel matrix evaluated on the training samples\n",
    "                fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "                ax[0].plot([i + 1 for i in range(len(plot_data[0]))], np.array(plot_data[2]), c=\"k\", marker=\"o\")\n",
    "                ax[0].set_xlabel(\"Iterations\")\n",
    "                ax[0].set_ylabel(\"Loss\")\n",
    "                ax[1].imshow(np.asmatrix(K), interpolation=\"nearest\", origin=\"upper\", cmap=\"Blues\")\n",
    "                ax[1].set_title(\"Optimized training kernel matrix\")\n",
    "                #fig.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Show performance values obtained by the theoretical (best) minimal point\n",
    "            index_min = np.argmin(plot_data[2])\n",
    "            min_loss_value = plot_data[2][index_min]\n",
    "            print(f\"QKA[minimal_point], loss={round(min_loss_value,4)}, params={plot_data[1][index_min]}\")\n",
    "            # Now train SVC with this configuration and check error\n",
    "            quant_kernel.assign_training_parameters(plot_data[1][index_min])\n",
    "            qsvc = QSVC(quantum_kernel=quant_kernel, C=best_C, class_weight=class_weight)\n",
    "\n",
    "            # Store loss values for statistics\n",
    "            loss_metrics = {\n",
    "                'baseline': loss_value_baseline,\n",
    "                'after_qkt': min_loss_value\n",
    "            }\n",
    "            \n",
    "            # Fit the QSVC\n",
    "            start = time.time()\n",
    "            qsvc.fit(xtrain_scaled, ytrain_rep)\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"QSVC + QKA[minimal_point] optimized kernel training time: {round(elapsed,2)} seconds\")\n",
    "            \n",
    "            # Evalaute accuracy\n",
    "            ypred_tr = qsvc.predict(xtrain_scaled)\n",
    "            print(f\"QSVC+QKA[minimal_point] (acc train): {round(metrics.accuracy_score(ytrain_rep, ypred_tr),2)}, (bal_acc train): {round(metrics.balanced_accuracy_score(ytrain_rep, ypred_tr),2)}, (kappa train): {round(metrics.cohen_kappa_score(ytrain_rep, ypred_tr),2)}\")\n",
    "            ypred_ts = qsvc.predict(xtest_scaled)\n",
    "            print(f\"QSVC+QKA[minimal_point] (acc test): {round(metrics.accuracy_score(ytest_rep, ypred_ts),2)}, (bal_acc test): {round(metrics.balanced_accuracy_score(ytest_rep, ypred_ts),2)}, (kappa test): {round(metrics.cohen_kappa_score(ytest_rep, ypred_ts),2)}\")\n",
    "            \n",
    "            # Print final kernel matrices\n",
    "            if minimize_display == False:\n",
    "                Kop_tr = quant_kernel.evaluate(xtrain_scaled)  # kernel matrix evaluated on the training samples\n",
    "                Kop_ts = quant_kernel.evaluate(x_vec=xtest_scaled, y_vec=xtrain_scaled)  # kernel matrix evaluated on the test samples\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                axs[0].imshow(np.asmatrix(Kop_tr), interpolation=\"nearest\", origin=\"upper\", cmap=\"Blues\")\n",
    "                axs[0].set_title(\"Final training kernel matrix\")\n",
    "                axs[1].imshow(np.asmatrix(Kop_ts), interpolation=\"nearest\", origin=\"upper\", cmap=\"Reds\")\n",
    "                axs[1].set_title(\"Final testing kernel matrix\")\n",
    "                plt.show()\n",
    "\n",
    "            # Store final results for this repetition\n",
    "            rep_metrics = compute_metrics(ytrain_rep, ypred_tr, ytest_rep, ypred_ts)\n",
    "            \n",
    "            if qfeat_map == \"Covariant\":\n",
    "                if qkt_single_training_parameter == True:\n",
    "                    results[\"qSVM_COV_opt_s\"].append(rep_metrics)\n",
    "                    results_loss[\"qSVM_COV_opt_s\"].append(loss_metrics)\n",
    "                else:\n",
    "                    results[\"qSVM_COV_opt_d\"].append(rep_metrics)\n",
    "                    results_loss[\"qSVM_COV_opt_d\"].append(loss_metrics)\n",
    "            elif qfeat_map == \"ZZ\":\n",
    "                if qkt_single_training_parameter == True:\n",
    "                    results[\"qSVM_ZZ_opt_s\"].append(rep_metrics)\n",
    "                    results_loss[\"qSVM_ZZ_opt_s\"].append(loss_metrics)\n",
    "                else:\n",
    "                    results[\"qSVM_ZZ_opt_d\"].append(rep_metrics)\n",
    "                    results_loss[\"qSVM_ZZ_opt_d\"].append(loss_metrics)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid quantum feature map\")\n",
    "\n",
    "    # Check point\n",
    "    save_results(filename_output, [results, results_hyper_params, results_loss])\n",
    "\n",
    "# END SCRIPT\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
